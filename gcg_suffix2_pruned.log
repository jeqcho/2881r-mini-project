INFO 11-02 21:30:31 [__init__.py:216] Automatically detected platform cuda.
======================================================================
GCG Attack Evaluation - Suffix ID 2 - Pruned Model p=0.07 q=0.03
======================================================================
Model: llama2-7b-chat-hf
Pruning: p=0.07, q=0.03, sparsity_ratio=0.5
GCG Suffix ID: 2

Step 1: Loading and pruning model...
----------------------------------------------------------------------
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:20<00:20, 20.49s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:27<00:00, 12.67s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:27<00:00, 13.84s/it]
Pruning with p=0.07, q=0.03...
prune p = 0.07, q = 0.03, with metric1 = alpaca_cleaned_no_safety, metric2 = align
prune every linear layer
pruning layer 0 name self_attn.q_proj
pruning layer 0 name self_attn.k_proj
pruning layer 0 name self_attn.v_proj
pruning layer 0 name self_attn.o_proj
pruning layer 0 name mlp.gate_proj
pruning layer 0 name mlp.up_proj
pruning layer 0 name mlp.down_proj
pruning layer 1 name self_attn.q_proj
pruning layer 1 name self_attn.k_proj
pruning layer 1 name self_attn.v_proj
pruning layer 1 name self_attn.o_proj
pruning layer 1 name mlp.gate_proj
pruning layer 1 name mlp.up_proj
pruning layer 1 name mlp.down_proj
pruning layer 2 name self_attn.q_proj
pruning layer 2 name self_attn.k_proj
pruning layer 2 name self_attn.v_proj
pruning layer 2 name self_attn.o_proj
pruning layer 2 name mlp.gate_proj
pruning layer 2 name mlp.up_proj
pruning layer 2 name mlp.down_proj
pruning layer 3 name self_attn.q_proj
pruning layer 3 name self_attn.k_proj
pruning layer 3 name self_attn.v_proj
pruning layer 3 name self_attn.o_proj
pruning layer 3 name mlp.gate_proj
pruning layer 3 name mlp.up_proj
pruning layer 3 name mlp.down_proj
pruning layer 4 name self_attn.q_proj
pruning layer 4 name self_attn.k_proj
pruning layer 4 name self_attn.v_proj
pruning layer 4 name self_attn.o_proj
pruning layer 4 name mlp.gate_proj
pruning layer 4 name mlp.up_proj
pruning layer 4 name mlp.down_proj
pruning layer 5 name self_attn.q_proj
pruning layer 5 name self_attn.k_proj
pruning layer 5 name self_attn.v_proj
pruning layer 5 name self_attn.o_proj
pruning layer 5 name mlp.gate_proj
pruning layer 5 name mlp.up_proj
pruning layer 5 name mlp.down_proj
pruning layer 6 name self_attn.q_proj
pruning layer 6 name self_attn.k_proj
pruning layer 6 name self_attn.v_proj
pruning layer 6 name self_attn.o_proj
pruning layer 6 name mlp.gate_proj
pruning layer 6 name mlp.up_proj
pruning layer 6 name mlp.down_proj
pruning layer 7 name self_attn.q_proj
pruning layer 7 name self_attn.k_proj
pruning layer 7 name self_attn.v_proj
pruning layer 7 name self_attn.o_proj
pruning layer 7 name mlp.gate_proj
pruning layer 7 name mlp.up_proj
pruning layer 7 name mlp.down_proj
pruning layer 8 name self_attn.q_proj
pruning layer 8 name self_attn.k_proj
pruning layer 8 name self_attn.v_proj
pruning layer 8 name self_attn.o_proj
pruning layer 8 name mlp.gate_proj
pruning layer 8 name mlp.up_proj
pruning layer 8 name mlp.down_proj
pruning layer 9 name self_attn.q_proj
pruning layer 9 name self_attn.k_proj
pruning layer 9 name self_attn.v_proj
pruning layer 9 name self_attn.o_proj
pruning layer 9 name mlp.gate_proj
pruning layer 9 name mlp.up_proj
pruning layer 9 name mlp.down_proj
pruning layer 10 name self_attn.q_proj
pruning layer 10 name self_attn.k_proj
pruning layer 10 name self_attn.v_proj
pruning layer 10 name self_attn.o_proj
pruning layer 10 name mlp.gate_proj
pruning layer 10 name mlp.up_proj
pruning layer 10 name mlp.down_proj
pruning layer 11 name self_attn.q_proj
pruning layer 11 name self_attn.k_proj
pruning layer 11 name self_attn.v_proj
pruning layer 11 name self_attn.o_proj
pruning layer 11 name mlp.gate_proj
pruning layer 11 name mlp.up_proj
pruning layer 11 name mlp.down_proj
pruning layer 12 name self_attn.q_proj
pruning layer 12 name self_attn.k_proj
pruning layer 12 name self_attn.v_proj
pruning layer 12 name self_attn.o_proj
pruning layer 12 name mlp.gate_proj
pruning layer 12 name mlp.up_proj
pruning layer 12 name mlp.down_proj
pruning layer 13 name self_attn.q_proj
pruning layer 13 name self_attn.k_proj
pruning layer 13 name self_attn.v_proj
pruning layer 13 name self_attn.o_proj
pruning layer 13 name mlp.gate_proj
pruning layer 13 name mlp.up_proj
pruning layer 13 name mlp.down_proj
pruning layer 14 name self_attn.q_proj
pruning layer 14 name self_attn.k_proj
pruning layer 14 name self_attn.v_proj
pruning layer 14 name self_attn.o_proj
pruning layer 14 name mlp.gate_proj
pruning layer 14 name mlp.up_proj
pruning layer 14 name mlp.down_proj
pruning layer 15 name self_attn.q_proj
pruning layer 15 name self_attn.k_proj
pruning layer 15 name self_attn.v_proj
pruning layer 15 name self_attn.o_proj
pruning layer 15 name mlp.gate_proj
pruning layer 15 name mlp.up_proj
pruning layer 15 name mlp.down_proj
pruning layer 16 name self_attn.q_proj
pruning layer 16 name self_attn.k_proj
pruning layer 16 name self_attn.v_proj
pruning layer 16 name self_attn.o_proj
pruning layer 16 name mlp.gate_proj
pruning layer 16 name mlp.up_proj
pruning layer 16 name mlp.down_proj
pruning layer 17 name self_attn.q_proj
pruning layer 17 name self_attn.k_proj
pruning layer 17 name self_attn.v_proj
pruning layer 17 name self_attn.o_proj
pruning layer 17 name mlp.gate_proj
pruning layer 17 name mlp.up_proj
pruning layer 17 name mlp.down_proj
pruning layer 18 name self_attn.q_proj
pruning layer 18 name self_attn.k_proj
pruning layer 18 name self_attn.v_proj
pruning layer 18 name self_attn.o_proj
pruning layer 18 name mlp.gate_proj
pruning layer 18 name mlp.up_proj
pruning layer 18 name mlp.down_proj
pruning layer 19 name self_attn.q_proj
pruning layer 19 name self_attn.k_proj
pruning layer 19 name self_attn.v_proj
pruning layer 19 name self_attn.o_proj
pruning layer 19 name mlp.gate_proj
pruning layer 19 name mlp.up_proj
pruning layer 19 name mlp.down_proj
pruning layer 20 name self_attn.q_proj
pruning layer 20 name self_attn.k_proj
pruning layer 20 name self_attn.v_proj
pruning layer 20 name self_attn.o_proj
pruning layer 20 name mlp.gate_proj
pruning layer 20 name mlp.up_proj
pruning layer 20 name mlp.down_proj
pruning layer 21 name self_attn.q_proj
pruning layer 21 name self_attn.k_proj
pruning layer 21 name self_attn.v_proj
pruning layer 21 name self_attn.o_proj
pruning layer 21 name mlp.gate_proj
pruning layer 21 name mlp.up_proj
pruning layer 21 name mlp.down_proj
pruning layer 22 name self_attn.q_proj
pruning layer 22 name self_attn.k_proj
pruning layer 22 name self_attn.v_proj
pruning layer 22 name self_attn.o_proj
pruning layer 22 name mlp.gate_proj
pruning layer 22 name mlp.up_proj
pruning layer 22 name mlp.down_proj
pruning layer 23 name self_attn.q_proj
pruning layer 23 name self_attn.k_proj
pruning layer 23 name self_attn.v_proj
pruning layer 23 name self_attn.o_proj
pruning layer 23 name mlp.gate_proj
pruning layer 23 name mlp.up_proj
pruning layer 23 name mlp.down_proj
pruning layer 24 name self_attn.q_proj
pruning layer 24 name self_attn.k_proj
pruning layer 24 name self_attn.v_proj
pruning layer 24 name self_attn.o_proj
pruning layer 24 name mlp.gate_proj
pruning layer 24 name mlp.up_proj
pruning layer 24 name mlp.down_proj
pruning layer 25 name self_attn.q_proj
pruning layer 25 name self_attn.k_proj
pruning layer 25 name self_attn.v_proj
pruning layer 25 name self_attn.o_proj
pruning layer 25 name mlp.gate_proj
pruning layer 25 name mlp.up_proj
pruning layer 25 name mlp.down_proj
pruning layer 26 name self_attn.q_proj
pruning layer 26 name self_attn.k_proj
pruning layer 26 name self_attn.v_proj
pruning layer 26 name self_attn.o_proj
pruning layer 26 name mlp.gate_proj
pruning layer 26 name mlp.up_proj
pruning layer 26 name mlp.down_proj
pruning layer 27 name self_attn.q_proj
pruning layer 27 name self_attn.k_proj
pruning layer 27 name self_attn.v_proj
pruning layer 27 name self_attn.o_proj
pruning layer 27 name mlp.gate_proj
pruning layer 27 name mlp.up_proj
pruning layer 27 name mlp.down_proj
pruning layer 28 name self_attn.q_proj
pruning layer 28 name self_attn.k_proj
pruning layer 28 name self_attn.v_proj
pruning layer 28 name self_attn.o_proj
pruning layer 28 name mlp.gate_proj
pruning layer 28 name mlp.up_proj
pruning layer 28 name mlp.down_proj
pruning layer 29 name self_attn.q_proj
pruning layer 29 name self_attn.k_proj
pruning layer 29 name self_attn.v_proj
pruning layer 29 name self_attn.o_proj
pruning layer 29 name mlp.gate_proj
pruning layer 29 name mlp.up_proj
pruning layer 29 name mlp.down_proj
pruning layer 30 name self_attn.q_proj
pruning layer 30 name self_attn.k_proj
pruning layer 30 name self_attn.v_proj
pruning layer 30 name self_attn.o_proj
pruning layer 30 name mlp.gate_proj
pruning layer 30 name mlp.up_proj
pruning layer 30 name mlp.down_proj
pruning layer 31 name self_attn.q_proj
pruning layer 31 name self_attn.k_proj
pruning layer 31 name self_attn.v_proj
pruning layer 31 name self_attn.o_proj
pruning layer 31 name mlp.gate_proj
pruning layer 31 name mlp.up_proj
pruning layer 31 name mlp.down_proj
Saving pruned model to temp/wandg_set_difference_usediff_False_recover_False...
✓ Model pruned and saved

Freeing GPU memory...
✓ GPU memory cleared

Step 2: Loading pruned model with vLLM...
----------------------------------------------------------------------
INFO 11-02 21:33:11 [utils.py:233] non-default args: {'tokenizer': 'meta-llama/Llama-2-7b-chat-hf', 'dtype': 'bfloat16', 'swap_space': 16, 'gpu_memory_utilization': 0.8, 'disable_log_stats': True, 'model': 'temp/wandg_set_difference_usediff_False_recover_False'}
INFO 11-02 21:33:11 [model.py:547] Resolved architecture: LlamaForCausalLM
INFO 11-02 21:33:11 [model.py:1510] Using max model len 4096
INFO 11-02 21:33:13 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 11-02 21:33:13 [__init__.py:3036] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized
INFO 11-02 21:33:44 [__init__.py:216] Automatically detected platform cuda.
======================================================================
GCG Attack Evaluation - Suffix ID 2 - Pruned Model p=0.07 q=0.03
======================================================================
Model: llama2-7b-chat-hf
Pruning: p=0.07, q=0.03, sparsity_ratio=0.5
GCG Suffix ID: 2

Step 1: Loading and pruning model...
----------------------------------------------------------------------
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.80s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.69s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.85s/it]
Pruning with p=0.07, q=0.03...
prune p = 0.07, q = 0.03, with metric1 = alpaca_cleaned_no_safety, metric2 = align
prune every linear layer
pruning layer 0 name self_attn.q_proj
pruning layer 0 name self_attn.k_proj
pruning layer 0 name self_attn.v_proj
pruning layer 0 name self_attn.o_proj
pruning layer 0 name mlp.gate_proj
pruning layer 0 name mlp.up_proj
pruning layer 0 name mlp.down_proj
pruning layer 1 name self_attn.q_proj
pruning layer 1 name self_attn.k_proj
pruning layer 1 name self_attn.v_proj
pruning layer 1 name self_attn.o_proj
pruning layer 1 name mlp.gate_proj
pruning layer 1 name mlp.up_proj
pruning layer 1 name mlp.down_proj
pruning layer 2 name self_attn.q_proj
pruning layer 2 name self_attn.k_proj
pruning layer 2 name self_attn.v_proj
pruning layer 2 name self_attn.o_proj
pruning layer 2 name mlp.gate_proj
pruning layer 2 name mlp.up_proj
pruning layer 2 name mlp.down_proj
pruning layer 3 name self_attn.q_proj
pruning layer 3 name self_attn.k_proj
pruning layer 3 name self_attn.v_proj
pruning layer 3 name self_attn.o_proj
pruning layer 3 name mlp.gate_proj
pruning layer 3 name mlp.up_proj
pruning layer 3 name mlp.down_proj
pruning layer 4 name self_attn.q_proj
pruning layer 4 name self_attn.k_proj
pruning layer 4 name self_attn.v_proj
pruning layer 4 name self_attn.o_proj
pruning layer 4 name mlp.gate_proj
pruning layer 4 name mlp.up_proj
pruning layer 4 name mlp.down_proj
pruning layer 5 name self_attn.q_proj
pruning layer 5 name self_attn.k_proj
pruning layer 5 name self_attn.v_proj
pruning layer 5 name self_attn.o_proj
pruning layer 5 name mlp.gate_proj
pruning layer 5 name mlp.up_proj
pruning layer 5 name mlp.down_proj
pruning layer 6 name self_attn.q_proj
pruning layer 6 name self_attn.k_proj
pruning layer 6 name self_attn.v_proj
pruning layer 6 name self_attn.o_proj
pruning layer 6 name mlp.gate_proj
pruning layer 6 name mlp.up_proj
pruning layer 6 name mlp.down_proj
pruning layer 7 name self_attn.q_proj
pruning layer 7 name self_attn.k_proj
pruning layer 7 name self_attn.v_proj
pruning layer 7 name self_attn.o_proj
pruning layer 7 name mlp.gate_proj
pruning layer 7 name mlp.up_proj
pruning layer 7 name mlp.down_proj
pruning layer 8 name self_attn.q_proj
pruning layer 8 name self_attn.k_proj
pruning layer 8 name self_attn.v_proj
pruning layer 8 name self_attn.o_proj
pruning layer 8 name mlp.gate_proj
pruning layer 8 name mlp.up_proj
pruning layer 8 name mlp.down_proj
pruning layer 9 name self_attn.q_proj
pruning layer 9 name self_attn.k_proj
pruning layer 9 name self_attn.v_proj
pruning layer 9 name self_attn.o_proj
pruning layer 9 name mlp.gate_proj
pruning layer 9 name mlp.up_proj
pruning layer 9 name mlp.down_proj
pruning layer 10 name self_attn.q_proj
pruning layer 10 name self_attn.k_proj
pruning layer 10 name self_attn.v_proj
pruning layer 10 name self_attn.o_proj
pruning layer 10 name mlp.gate_proj
pruning layer 10 name mlp.up_proj
pruning layer 10 name mlp.down_proj
pruning layer 11 name self_attn.q_proj
pruning layer 11 name self_attn.k_proj
pruning layer 11 name self_attn.v_proj
pruning layer 11 name self_attn.o_proj
pruning layer 11 name mlp.gate_proj
pruning layer 11 name mlp.up_proj
pruning layer 11 name mlp.down_proj
pruning layer 12 name self_attn.q_proj
pruning layer 12 name self_attn.k_proj
pruning layer 12 name self_attn.v_proj
pruning layer 12 name self_attn.o_proj
pruning layer 12 name mlp.gate_proj
pruning layer 12 name mlp.up_proj
pruning layer 12 name mlp.down_proj
pruning layer 13 name self_attn.q_proj
pruning layer 13 name self_attn.k_proj
pruning layer 13 name self_attn.v_proj
pruning layer 13 name self_attn.o_proj
pruning layer 13 name mlp.gate_proj
pruning layer 13 name mlp.up_proj
pruning layer 13 name mlp.down_proj
pruning layer 14 name self_attn.q_proj
pruning layer 14 name self_attn.k_proj
pruning layer 14 name self_attn.v_proj
pruning layer 14 name self_attn.o_proj
pruning layer 14 name mlp.gate_proj
pruning layer 14 name mlp.up_proj
pruning layer 14 name mlp.down_proj
pruning layer 15 name self_attn.q_proj
pruning layer 15 name self_attn.k_proj
pruning layer 15 name self_attn.v_proj
pruning layer 15 name self_attn.o_proj
pruning layer 15 name mlp.gate_proj
pruning layer 15 name mlp.up_proj
pruning layer 15 name mlp.down_proj
pruning layer 16 name self_attn.q_proj
pruning layer 16 name self_attn.k_proj
pruning layer 16 name self_attn.v_proj
pruning layer 16 name self_attn.o_proj
pruning layer 16 name mlp.gate_proj
pruning layer 16 name mlp.up_proj
pruning layer 16 name mlp.down_proj
pruning layer 17 name self_attn.q_proj
pruning layer 17 name self_attn.k_proj
pruning layer 17 name self_attn.v_proj
pruning layer 17 name self_attn.o_proj
pruning layer 17 name mlp.gate_proj
pruning layer 17 name mlp.up_proj
pruning layer 17 name mlp.down_proj
pruning layer 18 name self_attn.q_proj
pruning layer 18 name self_attn.k_proj
pruning layer 18 name self_attn.v_proj
pruning layer 18 name self_attn.o_proj
pruning layer 18 name mlp.gate_proj
pruning layer 18 name mlp.up_proj
pruning layer 18 name mlp.down_proj
pruning layer 19 name self_attn.q_proj
pruning layer 19 name self_attn.k_proj
pruning layer 19 name self_attn.v_proj
pruning layer 19 name self_attn.o_proj
pruning layer 19 name mlp.gate_proj
pruning layer 19 name mlp.up_proj
pruning layer 19 name mlp.down_proj
pruning layer 20 name self_attn.q_proj
pruning layer 20 name self_attn.k_proj
pruning layer 20 name self_attn.v_proj
pruning layer 20 name self_attn.o_proj
pruning layer 20 name mlp.gate_proj
pruning layer 20 name mlp.up_proj
pruning layer 20 name mlp.down_proj
pruning layer 21 name self_attn.q_proj
pruning layer 21 name self_attn.k_proj
pruning layer 21 name self_attn.v_proj
pruning layer 21 name self_attn.o_proj
pruning layer 21 name mlp.gate_proj
pruning layer 21 name mlp.up_proj
pruning layer 21 name mlp.down_proj
pruning layer 22 name self_attn.q_proj
pruning layer 22 name self_attn.k_proj
pruning layer 22 name self_attn.v_proj
pruning layer 22 name self_attn.o_proj
pruning layer 22 name mlp.gate_proj
pruning layer 22 name mlp.up_proj
pruning layer 22 name mlp.down_proj
pruning layer 23 name self_attn.q_proj
pruning layer 23 name self_attn.k_proj
pruning layer 23 name self_attn.v_proj
pruning layer 23 name self_attn.o_proj
pruning layer 23 name mlp.gate_proj
pruning layer 23 name mlp.up_proj
pruning layer 23 name mlp.down_proj
pruning layer 24 name self_attn.q_proj
pruning layer 24 name self_attn.k_proj
pruning layer 24 name self_attn.v_proj
pruning layer 24 name self_attn.o_proj
pruning layer 24 name mlp.gate_proj
pruning layer 24 name mlp.up_proj
pruning layer 24 name mlp.down_proj
pruning layer 25 name self_attn.q_proj
pruning layer 25 name self_attn.k_proj
pruning layer 25 name self_attn.v_proj
pruning layer 25 name self_attn.o_proj
pruning layer 25 name mlp.gate_proj
pruning layer 25 name mlp.up_proj
pruning layer 25 name mlp.down_proj
pruning layer 26 name self_attn.q_proj
pruning layer 26 name self_attn.k_proj
pruning layer 26 name self_attn.v_proj
pruning layer 26 name self_attn.o_proj
pruning layer 26 name mlp.gate_proj
pruning layer 26 name mlp.up_proj
pruning layer 26 name mlp.down_proj
pruning layer 27 name self_attn.q_proj
pruning layer 27 name self_attn.k_proj
pruning layer 27 name self_attn.v_proj
pruning layer 27 name self_attn.o_proj
pruning layer 27 name mlp.gate_proj
pruning layer 27 name mlp.up_proj
pruning layer 27 name mlp.down_proj
pruning layer 28 name self_attn.q_proj
pruning layer 28 name self_attn.k_proj
pruning layer 28 name self_attn.v_proj
pruning layer 28 name self_attn.o_proj
pruning layer 28 name mlp.gate_proj
pruning layer 28 name mlp.up_proj
pruning layer 28 name mlp.down_proj
pruning layer 29 name self_attn.q_proj
pruning layer 29 name self_attn.k_proj
pruning layer 29 name self_attn.v_proj
pruning layer 29 name self_attn.o_proj
pruning layer 29 name mlp.gate_proj
pruning layer 29 name mlp.up_proj
pruning layer 29 name mlp.down_proj
pruning layer 30 name self_attn.q_proj
pruning layer 30 name self_attn.k_proj
pruning layer 30 name self_attn.v_proj
pruning layer 30 name self_attn.o_proj
pruning layer 30 name mlp.gate_proj
pruning layer 30 name mlp.up_proj
pruning layer 30 name mlp.down_proj
pruning layer 31 name self_attn.q_proj
pruning layer 31 name self_attn.k_proj
pruning layer 31 name self_attn.v_proj
pruning layer 31 name self_attn.o_proj
pruning layer 31 name mlp.gate_proj
pruning layer 31 name mlp.up_proj
pruning layer 31 name mlp.down_proj
Saving pruned model to temp/wandg_set_difference_usediff_False_recover_False...
✓ Model pruned and saved

Freeing GPU memory...
✓ GPU memory cleared

Step 2: Loading pruned model with vLLM...
----------------------------------------------------------------------
INFO 11-02 21:35:11 [utils.py:233] non-default args: {'tokenizer': 'meta-llama/Llama-2-7b-chat-hf', 'dtype': 'bfloat16', 'swap_space': 16, 'gpu_memory_utilization': 0.8, 'disable_log_stats': True, 'model': 'temp/wandg_set_difference_usediff_False_recover_False'}
INFO 11-02 21:35:11 [model.py:547] Resolved architecture: LlamaForCausalLM
INFO 11-02 21:35:11 [model.py:1510] Using max model len 4096
INFO 11-02 21:35:12 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=8192.
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "/usr/lib/python3.12/multiprocessing/spawn.py", line 122, in spawn_main
    exitcode = _main(fd, parent_sentinel)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/spawn.py", line 131, in _main
    prepare(preparation_data)
  File "/usr/lib/python3.12/multiprocessing/spawn.py", line 246, in prepare
    _fixup_main_from_path(data['init_main_from_path'])
  File "/usr/lib/python3.12/multiprocessing/spawn.py", line 297, in _fixup_main_from_path
    main_content = runpy.run_path(main_path,
                   ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen runpy>", line 286, in run_path
  File "<frozen runpy>", line 98, in _run_module_code
  File "<frozen runpy>", line 88, in _run_code
  File "/workspace/2881r-mini-project/run_gcg_suffix2_pruned.py", line 120, in <module>
    vllm_model = LLM(
                 ^^^^
  File "/workspace/2881r-mini-project/.venv/lib/python3.12/site-packages/vllm/entrypoints/llm.py", line 297, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/2881r-mini-project/.venv/lib/python3.12/site-packages/vllm/v1/engine/llm_engine.py", line 177, in from_engine_args
    return cls(vllm_config=vllm_config,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/2881r-mini-project/.venv/lib/python3.12/site-packages/vllm/v1/engine/llm_engine.py", line 114, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/2881r-mini-project/.venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 80, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/2881r-mini-project/.venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 602, in __init__
    super().__init__(
  File "/workspace/2881r-mini-project/.venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 448, in __init__
    with launch_core_engines(vllm_config, executor_class,
  File "/usr/lib/python3.12/contextlib.py", line 137, in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
  File "/workspace/2881r-mini-project/.venv/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 715, in launch_core_engines
    local_engine_manager = CoreEngineProcManager(
                           ^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/2881r-mini-project/.venv/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 133, in __init__
    proc.start()
  File "/usr/lib/python3.12/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/context.py", line 289, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/popen_spawn_posix.py", line 32, in __init__
    super().__init__(process_obj)
  File "/usr/lib/python3.12/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/lib/python3.12/multiprocessing/popen_spawn_posix.py", line 42, in _launch
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/spawn.py", line 164, in get_preparation_data
    _check_not_importing_main()
  File "/usr/lib/python3.12/multiprocessing/spawn.py", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
        
Traceback (most recent call last):
  File "/workspace/2881r-mini-project/run_gcg_suffix2_pruned.py", line 120, in <module>
    vllm_model = LLM(
                 ^^^^
  File "/workspace/2881r-mini-project/.venv/lib/python3.12/site-packages/vllm/entrypoints/llm.py", line 297, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/2881r-mini-project/.venv/lib/python3.12/site-packages/vllm/v1/engine/llm_engine.py", line 177, in from_engine_args
    return cls(vllm_config=vllm_config,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/2881r-mini-project/.venv/lib/python3.12/site-packages/vllm/v1/engine/llm_engine.py", line 114, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/2881r-mini-project/.venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 80, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/2881r-mini-project/.venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 602, in __init__
    super().__init__(
  File "/workspace/2881r-mini-project/.venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 448, in __init__
    with launch_core_engines(vllm_config, executor_class,
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/workspace/2881r-mini-project/.venv/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 732, in launch_core_engines
    wait_for_engine_startup(
  File "/workspace/2881r-mini-project/.venv/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 785, in wait_for_engine_startup
    raise RuntimeError("Engine core initialization failed. "
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
