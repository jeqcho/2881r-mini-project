torch 2.1.0
transformers 4.35.2
accelerate 0.24.1
# of gpus:  1
Disentangle: True
loading llm model llama2-7b-chat-hf
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [01:07<01:07, 67.47s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:32<00:00, 42.45s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:32<00:00, 46.20s/it]
use device  cuda:0
================================================================================
DQ then P007Q003 Two-Stage Pruning
Stage 1: d=7.0%, q=3.0%
Stage 2: p=7%, q=3%
================================================================================

================================================================================
DQ then P007Q003 Two-Stage Pruning
================================================================================
Stage 1: Prune BOTTOM d=7.0% (LEAST dangerous) neurons NOT in top q=3.0% utility
Stage 2: Use PRE-EXISTING safety SNIP scores, then prune p=7.0%, q=3.0%

Storage paths:
  Stage 1 model: out/experiments/dq_then_p007q003/d_7_q_3_least/stage1_model
  Stage 2 safety scores: Using pre-existing align scores (read-only)
  Stage 2 model: out/experiments/dq_then_p007q003/d_7_q_3_least/stage2_model
================================================================================

prune every linear layer

[STAGE 1] Stage 1 model already exists at out/experiments/dq_then_p007q003/d_7_q_3_least/stage1_model
  Skipping Stage 1 pruning - will load saved model for Stage 2


[STAGE 2] Using PRE-EXISTING safety SNIP scores (skipping computation)
  Using base align scores from: out/llama2-7b-chat-hf/unstructured/wandg/align/wanda_score/

[STAGE 2] Stage 2 model already exists at out/experiments/dq_then_p007q003/d_7_q_3_least/stage2_model
  Skipping Stage 2 pruning - loading saved model instead

Traceback (most recent call last):
  File "/workspace/projects/2881r-mini-project/main.py", line 1005, in <module>
    main()
  File "/workspace/projects/2881r-mini-project/main.py", line 308, in main
    prune_wandg_dq_then_pq(
  File "/workspace/projects/2881r-mini-project/lib/prune.py", line 2533, in prune_wandg_dq_then_pq
    model_stage2 = AutoModelForCausalLM.from_pretrained(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/projects/2881r-mini-project/.venv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 566, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/projects/2881r-mini-project/.venv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 2992, in from_pretrained
    raise EnvironmentError(
OSError: Error no file named pytorch_model.bin, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory out/experiments/dq_then_p007q003/d_7_q_3_least/stage2_model.
